{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 1\n",
    "# 1a\n",
    "![](Figure_1.png)\n",
    "\n",
    "## Table of Contents\n",
    "- [Derivatives](#1b2)\n",
    "- [Training with different functions](#1e1)\n",
    "- [Training with different numbers of hidden neurons](#1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b2\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "$\\sigma'(z) = \\frac{\\partial}{\\partial z}\\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "$\\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "$\\frac{1 + e^{-z}}{(1 + e^{-z})^2} - (\\frac{1}{1 + e^{-z}})^2$\n",
    "\n",
    "$\\frac{1}{(1 + e^{-z})} - (\\frac{1}{1 + e^{-z}})^2$\n",
    "\n",
    "$\\sigma(z) - \\sigma(z)^2$\n",
    "\n",
    "$\\sigma(z) * (1 - \\sigma(z)$\n",
    "\n",
    "\n",
    "## Tanh\n",
    "\n",
    "$f'(z) = \\frac{\\text{sinh'}(z) \\times \\text{cosh}(z) - \\text{cosh'}(z) \\times \\text{sinh}(z)}{\\text{cosh}^2(z)}$\n",
    "\n",
    "$\\frac{\\text{cosh}^2(z) - \\text{sinh}^2(z)}{\\text{cosh}^2(z)}$\n",
    "\n",
    "$1 - \\frac{\\text{sinh}^2(z)}{\\text{cosh}^2(z)}$\n",
    "\n",
    "$1 - \\text{tanh}^2(z)$\n",
    "\n",
    "\n",
    "## ReLU\n",
    "\n",
    "1 if z > 0, otherwise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1e1\n",
    "\n",
    "## Tanh\n",
    "![](Figure_1e1_tanh.png)\n",
    "\n",
    "## Sigmoid\n",
    "![](Figure_1e1_sigmoid.png)\n",
    "\n",
    "## ReLU\n",
    "![](Figure_1e1_relu.png)\n",
    "\n",
    "Tanh and sigmoid activation functions behave similarly with a smooth decision boundary on each of them. The ReLU is quite jagged. This is probably because the ReLU function itself is linear and the decision boundary is composed of these ReLU functions. There are 3 points at which the line changes angles which if I were to guess would be the 3 hidden neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1e2\n",
    "For all, I am using ReLU\n",
    "## 5\n",
    "![](Figure_1e2_5.png)\n",
    "I don't know, but this looks worse than 3.\n",
    "\n",
    "## 10\n",
    "![](Figure_1e2_10.png)\n",
    "This starts to capture points that are out of the way/buried in the other points.\n",
    "\n",
    "## 50\n",
    "![](Figure_1e2_50.png)\n",
    "From this point onward, it doesn't seem to improve in performance as you add more hidden neurons.\n",
    "\n",
    "## 100\n",
    "![](Figure_1e2_100.png)\n",
    "\n",
    "## 500\n",
    "![](Figure_1e2_500.png)\n",
    "In general, as more hidden neurons are added, the more the decision boundary can bend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer class needs to have the attributes:\n",
    "- all of the activations\n",
    "- all of the z\n",
    "- all of the W, b and the gradients of those as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
